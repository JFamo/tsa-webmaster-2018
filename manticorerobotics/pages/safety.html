<html lang="en"><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Oxygen|Ubuntu:500" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=PT+Sans" rel="stylesheet">
	<!-- jquery -->
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <!-- Bootstrap, cause it dabs -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.3/css/bootstrap.min.css" integrity="sha384-Zug+QiDoJOrZ5t4lssLdxGhVrurbmBWopoEl+M6BdEfwnCJZtKxi1KgxUyJq13dy" crossorigin="anonymous">
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
	<!-- Grab my css and js -->
    <link rel="stylesheet" href="../css/style.css">
  </head>
  <body>

  	<!-- so peeps dont get lost -->
  	<nav class="navbar navbar-dark bg-primary navbar-expand-sm">
  	<div class="container-fluid">
	    <a class="navbar-brand" href="#">Manticore Robotics</a>
	    <div class="ml-auto navbar-nav">
	    	<a class="nav-item nav-link" href="../index.html">Home</a>
	      	<a class="nav-item nav-link" href="existing.html">Existing Tech</a>
	      	<a class="nav-item nav-link" href="costs.html">Costs</a>
	      	<a class="nav-item nav-link" href="uses.html">Uses</a>
	      	<a class="nav-item nav-link" href="technology.html">Technology</a>
	      	<a class="nav-item nav-link" href="benefits.html">Benefits</a>
	      	<a class="nav-item nav-link active" href="safety.html">Safety</a>
	      	<a class="nav-item nav-link" href="sources.html">Sources</a>
	      	<a class="nav-item nav-link" href="http://bashtsa.x10host.com">BASHTSA</a>
	    </div>
	</div>
	</nav>

  	<!-- the actual things -->
  	<div class="container-fluid">
	  	<div class="row">
		  	<div class="col-sm-12">
		  		<center>
			  	<div class="pageTitle">
			  		<p class="titleTextType1 text-primary" id="titleTextStagger1">Safety Concerns and Liabilities</p>
			  	</div>
			  	<div class="contentBox" style="text-align:left;">
			  		<p class="detailsText" id="titleDetailsText">Despite media speculation, Manticore Robotics ensures all prospective buyers that Plexor units are fully regulated by the federal government, pose no threat to owners, and that all information secured through Plexor Homebots is kept secure and private. Plexor is programmed to totally avoid malicious action, and will disregard any violence encouraged by users. Manticore Robotics is not liable for any damages to Plexor the Homebot resulting from user commands, or injury to users resulting from user actions. The artifical intelligence industry has faced various speculations and criticisms over safety and liabilities over the past few years. Here are some concerns expressed by the public, which Manticore Robotics would like to ensure readers it has totally avoided.</p>
			  	</div>
			  	<div class="contentBox" style="text-align:left;">
			  		<p class="titleTextType2">Morals</p>
			  		<p class="detailsText" id="titleDetailsText">If AI is to exist and operate autonomously in everyday utilities, like cars, they must be able to make morally sound decisions on their own. In the case of vehicles, consider this: a child on a bicycle swerves in front of an automated car. Should the car veer into the next lane, where it may hit a car, or turn onto the grass, where it may hit a tree? Or, should the car hit the child on the bike? Scenarios like these are exemplified in a moral decision tester known as the “Moral Machine” by MIT, and are unfortunately faced by people every day. Thus, when AI take control of out cars, they must be able to make the best ethical decision. 
<br><br>In order to do this, developers need to have people make ethical choices (as seen in the “Moral Machine”), then let machine learning discern a pattern in said choices. These patterns can then be translated into AI systems. Such developers, like Professeur Vincent Conitzer from Duke, also worry that today’s society may have not reached its “apex” in morality: racism and sexism are still issues that prevail in people’s thoughts. If such biases remained within AI, it would not truly be “safe”.
</p>
			  	</div>
			  	<div class="contentBox" style="text-align:left;">
			  		<p class="titleTextType2">Supervision</p>
			  		<p class="detailsText" id="titleDetailsText">The goal of supervising an AI is to make it sure it conforms to the interests of humans. Its actions must be evaluated and understood by people, only then can an AI system be controlled. Such evaluation, unfortunately, takes an incredibly long time with more complex and intelligent systems. It could take an engineer three hours to understand a decision made by a bot in one second, making the evaluation of all the bot’s decisions a nightmare to think about. Thus, researchers only look at specific “actions of interest” performed by the AI, which may involve user interaction. If this is the case, the user is usually contacted via a survey to collect feedback; to assess whether a bot should interrupt a phone call to send a notification is an example of something researchers would ask the user for.
<br><br>In many cases, like that of IBM’s Watson, AI can become smarter than humans. If this is the case, there is no way a human could understand the actions of such a bot on his or her own; it simply knows more than anyone supervising it. Any bot that competes in games like chess or Go is another example: it would require a master at the game to understand the actions of a complex AI playing the same game, and even then, he or she may not understand its choices till the end of the algorithm’s strategy. The solution to such an issue is an ironic one: let the complex AI system be evaluated by a less complex AI system driven by humans. Unlike researchers and engineers, an AI can collect and comprehend data, as well as advise and revise the superior AI’s behavior, much faster. In the future, that superior AI could then be used as a training tool for even more complex AIs, making sure that no matter how smart they are, they will always conform to human interests and not its own. </p>
			  	</div>
			  	<div class="contentBox" style="text-align:left;">
			  		<p class="titleTextType2">Liability</p>
			  		<div class="row">
    					<div class="col-sm-8">
			  				<p class="detailsText" id="titleDetailsText">If a self driving car hits a person, or an autonomous weapon kills innocents, who is to blame? The manufacturer, the programmer, or the robot? Such questions can be very complicated, especially when it involves organizations like the US Military. In the case of an autonomous weapon, normally the soldier that commits war crimes is the one who is held responsible, but if the weapon could not fully distinguish innocents from enemies, how could anyone blame it? It’s like blaming a computer for freezing itself and deleting the user’s unsaved information. It did not intend on harming the user’s documents in the same way that the autonomous weapon did not intend on killing innocents. 
<br><br>In a simpler case, such as that of an self-driving car, the manufacturer is normally blamed for any issue regarding the actions of the vehicle. Too, most manufacturers like Volvo and Tesla are ready to accept the blame if it ever comes. It has been suggested to the US Military to create an attachment between autonomous weapons and commanders, in which the commanders can demonstrate their control over the weapon before battle, so that they may take responsibility for the bot’s actions. AI Ethics conferences are currently being held to evaluate these legal issues and the concerns over AI liability. For now, the incentive of avoiding class-action lawsuits are making sure that manufacturers put general safety over anything when it comes to AI. </p>
			  			</div>
			  			<div class="col-sm-4">
			  				<img class="img-rounded contentPic" src="../img/ethics.png" alt="Content Image" />
			  			</div>
			  		</div>
			  	</div>
			  	<div class="contentBox" style="text-align:left;">
			  		<p class="titleTextType2">Probability</p>
			  				<p class="detailsText" id="titleDetailsText">As discussed before, autonomous AI must make decisions that conform to human interests and safety. However, when it’s on its own and needs to make relatively quick actions, it relies on probability. For example, if a self-driving car finds an aggressive driver behind it and is reaching a turn or a stoplight, it must determine the probability of the actions of that driver and act accordingly. But in order for this to work, its probability calculations must be accurate and not confuse 10% with 0.1%, which may result in erroneous actions.
<br><br>In order for AI to calculate probability safely, it needs to collect a lot of accurate data, and it needs to be able to understand and draw conclusions from said data correctly. Whether it be some black ice or a pothole, the AI must know enough about any situation to be able to hypothesize a safe solution. The difficult thing about collecting such data is that, as it expands, the AI’s reasoning systems must improve as well in order to incorporate more variables. Thus, improving probability analysis is one of the key developments for AI safety, and is being worked on for almost all autonomous AI, especially uncontrolled cars and drones. 
</p>
			  	</div>
			  	<div class="contentBox" style="text-align:left;">
			  		<p class="titleTextType2">Silo Busting</p>
			  		<p class="detailsText" id="titleDetailsText">Any AI must have a strong ethics system that focuses on human values, interests, and safety. While computer scientists are hard at work trying to implement these values into their AI systems, they can never be able to have a fully ethical AI on their own. Professionals from other fields, such as social scientists, are required to ensure that robots act safely. Scientists of the same profession that work on the same project alone are known to be working in “silos”. In the case of AI, this can be dangerous, since professionals in other fields related to human thought processes can not only improve the safety of AI decision making, but they can also help design complex neural networks (which are designed after the human brain). 
<br><br>Organizations like The Hastings Center are trying to bring leaders in diverse fields of study to come together and collaborate on AI. These “silo-busters” are being funded and supported by the UN and the World Economic Forum to help ensure that AI and robots are being created to benefit human society effectively and safely. This is a difficult process, since it takes time for scientists with different “languages” to understand each other intellectually; computer scientists may say that building complex morals into a system is doable (due to their knowledge of AI), while ethics professionals may think otherwise (due to their knowledge of the human mind). Even so, with more time spent into these silo-busting workshops, professionals are starting to collaborate more and more, and AI is starting to get looked at by more than just computer programmers and scientists.
</p>
			  	</div>
			  	<div class="contentBox" style="text-align:left;">
			  		<p class="titleTextType2">Financial Manipulation</p>
			  		<div class="row">
    					<div class="col-sm-8">
			  		<p class="detailsText" id="titleDetailsText">Many financial companies use AI agents to help conduct trades and investments, due to its superior speed and accuracy compared to any human. These financial AI systems control “over half of trading volume in US equities” today according to Michael Wellman in Minds and Machines. A major concern with this level of AI involvement in markets is the possibility of the system messing up, as seen with Knight Capital’s loss of $440 million due to a simple malfunction, although such cases are rare and require a better understanding of AI agents. The more experience AI receives in these financial ventures, the better it gets at them, which worries many financial analysts since these agents only get better and better at manipulating the market to gain profits.
<br><br>Exploitation of arbitrage and “spoofing” are only a few of the many manipulation techniques practiced by AI agents. Arbitrage is the act of purchasing an asset in one market and selling it in another market at an increased value. This may seem like a fair play in the financial world, and it is, however such actions are done due to market inefficiencies. These agents, on the other hand, are beginning to purposefully cause these discrepancies for profit at a rapid pace, deteriorating the market’s overall vitality with it. Spoofing is an equally damaging act in which the bot bids for a stock item, then cancels the bid before it executes. Currently, the government does not have the technical capability to regulate these bots, and due to the secrecy of the financial world, progress on preventing these abuses are slow. However, through reverse engineering, professionals like Wellman plan to figure out the mystery to these erroneous behaviors and hopefully redesign the market to be safer in the future.
</p>
					</div>
					<div class="col-sm-4">
			  			<img class="img-rounded contentPic" src="../img/finance.jpg" alt="Content Image" />
			  		</div>
					</div>
			  	</div>
			  	</center>
		  	</div>
		</div>
	</div>

	<br><br><br>
	
	<nav aria-label="Page navigation example">
	  <ul class="pagination justify-content-center">
	    <li class="page-item"><a href="benefits.html" class="page-link">Benefits</a></li>
	    <li class="page-item"><a class="page-link" href="sources.html">Sources</a></li>
	  </ul>
	</nav>
  
  <footer>
  	<!-- get js late -->
  	<script src="../js/script.js"></script>
  </footer>
</body></html>